# =============================================================================
# Knowledge Base Configuration
# =============================================================================
# Copy this file to .env and fill in your values
# Environment variables override these file values (12-factor app pattern)

# =============================================================================
# LLM Provider Configuration
# =============================================================================
# The system supports multiple LLM providers through a pluggable architecture.
# New providers can be added by implementing BaseLLM and registering with
# @register_provider("provider_name") in the factory.
#
# Currently supported providers:
#   - claude    : Anthropic Claude API (cloud)
#   - ollama    : Local Ollama server
#   - (extensible: add OpenAI, Gemini, etc. by implementing BaseLLM)
#
# Provider Selection:
#   - Set to a specific provider name, or
#   - Leave empty for auto-select (uses Claude if API key exists, else Ollama)
LLM_PROVIDER=claude

# -----------------------------------------------------------------------------
# Claude/Anthropic Settings (when LLM_PROVIDER=claude)
# -----------------------------------------------------------------------------
ANTHROPIC_API_KEY=sk-ant-api03-your-key-here
ANTHROPIC_MODEL=claude-3-5-haiku-20241022

# -----------------------------------------------------------------------------
# Ollama Settings (when LLM_PROVIDER=ollama)
# -----------------------------------------------------------------------------
OLLAMA_BASE_URL=http://ollama:11434
OLLAMA_LLM_MODEL=llama3.1:8b

# -----------------------------------------------------------------------------
# Future Provider Settings (examples for extensibility)
# -----------------------------------------------------------------------------
# When adding new providers, add their configuration here:
# OPENAI_API_KEY=sk-your-openai-key
# OPENAI_MODEL=gpt-4o-mini
# GEMINI_API_KEY=your-gemini-key
# GEMINI_MODEL=gemini-pro

# Common LLM Settings
METADATA_BATCH_SIZE=10

# =============================================================================
# Confluence Integration
# =============================================================================
CONFLUENCE_URL=https://your-domain.atlassian.net
CONFLUENCE_USERNAME=your-email@company.com
CONFLUENCE_API_TOKEN=your-api-token
CONFLUENCE_SPACE_KEYS=ENG,HR,DOCS

# =============================================================================
# Slack Bot
# =============================================================================
SLACK_BOT_TOKEN=xoxb-your-bot-token
SLACK_SIGNING_SECRET=your-signing-secret
SLACK_APP_TOKEN=xapp-your-app-token

# =============================================================================
# Infrastructure (usually leave defaults for Docker)
# =============================================================================
CHROMA_HOST=chromadb
CHROMA_PORT=8000
REDIS_URL=redis://redis:6379/0
DATABASE_URL=sqlite+aiosqlite:///./knowledge_base.db
